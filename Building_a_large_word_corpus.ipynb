{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM478HuLFTgUUfaoq6ae5kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaledai069/Crossword-Solver-A-Neural-Transformer-based-Approach/blob/master/Building_a_large_word_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJwrScIE2KUN",
        "outputId": "85f0ddd5-5b3d-4603-a327-8792b553283f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "V5HheyUa8iRT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('abc')\n",
        "nltk.download('genesis')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('state_union')\n",
        "nltk.download('webtext')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTwEafin2QwH",
        "outputId": "d8a605b6-7c4d-45e0-b3ca-2ff023bb5e25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import inaugural, wordnet, words, abc, genesis, gutenberg, state_union, webtext, cmudict, brown\n",
        "\n",
        "all_corporas = [inaugural, wordnet, words, abc, genesis, gutenberg, state_union, webtext, cmudict, brown]\n",
        "overall_word_list = []\n",
        "for corpora in all_corporas:\n",
        "  corpora_name = str(corpora).split('corpora/')[-1].split(\"'\")[0]\n",
        "  total_words = len(set(corpora.words()))\n",
        "  print(f\"CORPORA: {corpora_name} WORDS COUNT: {total_words}\")\n",
        "\n",
        "  overall_word_list.extend(list(set(corpora.words())))\n",
        "\n",
        "print(\"Total Words: \", len(overall_word_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsoYDYyQ5DK_",
        "outputId": "1ef9011d-e521-4c91-d456-95d35a30ac75"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CORPORA: inaugural WORDS COUNT: 10025\n",
            "CORPORA: wordnet.zip/wordnet/ WORDS COUNT: 147306\n",
            "CORPORA: words WORDS COUNT: 235892\n",
            "CORPORA: abc WORDS COUNT: 31885\n",
            "CORPORA: genesis WORDS COUNT: 25841\n",
            "CORPORA: gutenberg WORDS COUNT: 51156\n",
            "CORPORA: state_union WORDS COUNT: 14591\n",
            "CORPORA: webtext WORDS COUNT: 21538\n",
            "CORPORA: cmudict WORDS COUNT: 123455\n",
            "CORPORA: brown WORDS COUNT: 56057\n",
            "Total Words:  717746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After merging all the plaintext corpus:\", len(set(overall_word_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W-Ar1d_7OEO",
        "outputId": "7354ae10-02dd-45bd-befb-d680034e4581"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After merging all the plaintext corpus: 492435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All unique words that can be collected from the nltk.corpus"
      ],
      "metadata": {
        "id": "nPub-jql7pSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collected_unique_words = list(set(overall_word_list))"
      ],
      "metadata": {
        "id": "4tRvgw8i7olR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assessing the BCS build words_alph.txt"
      ],
      "metadata": {
        "id": "5I4Lqulo73Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = list(set([a.strip() for a in open('/content/words_alpha.txt','r').readlines()]))\n",
        "\n",
        "for word in tqdm(dictionary, ncols = 100):\n",
        "  for char in word.lower():\n",
        "    if char not in string.ascii_lowercase:\n",
        "      print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnZwPL8w711U",
        "outputId": "bac48780-6523-43b5-ff47-2f0c0ba01e00"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████| 370103/370103 [00:03<00:00, 115386.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, there are no words in the dictionary with hypen and any other characters"
      ],
      "metadata": {
        "id": "e5Yy8Ue_891R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_valid_words = []\n",
        "invalid_words = []\n",
        "invalid_char_found = False\n",
        "\n",
        "for word in tqdm(collected_unique_words, ncols = 120):\n",
        "  for char in word.lower():\n",
        "    if char not in string.ascii_lowercase:\n",
        "      invalid_char_found = True\n",
        "      break\n",
        "  if invalid_char_found:\n",
        "    invalid_char_found = False\n",
        "    invalid_words.append(word.lower())\n",
        "  else:\n",
        "    unique_valid_words.append(word.lower())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYrfB9WR89TA",
        "outputId": "05d06af2-c7d2-4044-b1ad-6490ba627cee"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████| 492435/492435 [00:02<00:00, 203805.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words = []\n",
        "\n",
        "for word in invalid_words:\n",
        "  modified_sentence = word.replace('-', ' ').replace('_', ' ').replace(\"'\", '')\n",
        "  possible_words = modified_sentence.split(' ')\n",
        "\n",
        "  invalid_char_found_1 = False\n",
        "  for w in possible_words:\n",
        "    for char in w.lower():\n",
        "      if char not in string.ascii_lowercase:\n",
        "        invalid_char_found_1 = True\n",
        "        break\n",
        "\n",
        "    if invalid_char_found_1:\n",
        "      invalid_char_found_1 = False\n",
        "    else:\n",
        "      filtered_words.append(w.lower())\n",
        "\n",
        "filtered_words = list(set(filtered_words))\n",
        "len(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8J-dDE8-W0T",
        "outputId": "9ab23b1a-1d0e-457c-9ae8-85099fb0546f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42264"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collected_valid_words = list(set(unique_valid_words + filtered_words))\n",
        "\n",
        "print(\"Unique words length: \", len(unique_valid_words))\n",
        "print(\"Words from invalid list length: \", len(filtered_words))\n",
        "print(\"All collected unique valid words: \", len(collected_valid_words))\n",
        "print(\"BCS words dictionary lenght: \", len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsWIu2RN_wmj",
        "outputId": "d62552a6-f703-403a-a497-fa1f102cd90e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words length:  399224\n",
            "Words from invalid list length:  42264\n",
            "All collected unique valid words:  373995\n",
            "BCS words dictionary lenght:  370103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, I have successfully collected about 373995 unique valid words, whereas the dictionary build by BCS has 370103 words."
      ],
      "metadata": {
        "id": "V26Vb7HZBYqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assess the overlap between BCS build dictionary and out built dictionary"
      ],
      "metadata": {
        "id": "4RGjKODTCH3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "own_collected = set(collected_valid_words)\n",
        "bcs_collected = set(dictionary)\n",
        "\n",
        "len(own_collected.intersection(bcs_collected))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-V9qOHOBWS4",
        "outputId": "2825b7a0-5af1-4f13-90cc-d309b87cd20b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "268623"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_dictionary = own_collected.union(bcs_collected)\n",
        "sorted_words_dictionary = sorted(list(words_dictionary))\n",
        "print(\"Total words in the dictionary: \", len(words_dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gEC_YhnBVq6",
        "outputId": "b984ec4c-b6ee-479e-c357-b1412a8d5ffe"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the dictionary:  475475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/new_words_alpha.txt'\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "  f.write(\"\\n\".join(list(sorted_words_dictionary)))"
      ],
      "metadata": {
        "id": "ilRohhGeD9Z7"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}